import cv2
import numpy as np
import os
from matplotlib import pyplot as plt
import time
import mediapipe as mp
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense
from tensorflow.keras.callbacks import TensorBoard
from scipy import stats
from sklearn.model_selection import train_test_split
from tensorflow.keras.utils import to_categorical
import tensorflow as tf

mp_holistic = mp.solutions.holistic  # Holistic model
mp_drawing = mp.solutions.drawing_utils 
actions = np.array(['accident', 'hungry', 'disaster', 'toilet', 'medicine'])

def mediapipe_detection(image, model):
    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert color space
    image.flags.writeable = False                   # Set non-writeable for efficiency
    results = model.process(image)                  # Make prediction
    image.flags.writeable = True                    # Set writeable
    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)  # Convert back to BGR
    return image, results

def draw_landmarks(image, results):
    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS)
    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS)
    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS)

def extract_keypoints(results):
    pose = np.array([[res.x, res.y, res.z] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33 * 3)
    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21 * 3)
    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21 * 3)
    return np.concatenate([pose, lh, rh])


LEARNING_RATE = 0.03
log_dir = os.path.join('Logs')
tb_callback = TensorBoard(log_dir=log_dir)

model=None
model = Sequential()
model.add(LSTM(256, return_sequences=True, activation='relu', input_shape=(30, 225)))
model.add(LSTM(128, return_sequences=True, activation='relu'))
model.add(LSTM(64, return_sequences=False, activation='relu'))
model.add(Dense(64, activation='relu'))
model.add(Dense(32, activation='relu'))
model.add(Dense(actions.shape[0], activation='softmax'))

model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE), loss='categorical_crossentropy', metrics=['categorical_accuracy'])




colors = [(245, 117, 16), (117, 245, 16), (16, 117, 245), (245, 16, 117), (16, 245, 245)]

def prob_viz(res, actions, input_frame, colors):
    output_frame = input_frame.copy()
    for num, prob in enumerate(res):
        cv2.rectangle(output_frame, (0, 60 + num * 40), (int(prob * 100), 90 + num * 40), colors[num], -1)
        cv2.putText(output_frame, f'{actions[num]}: {prob:.2f}', (5, 80 + num * 40), 
                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)
    return output_frame
sequence = []
predictions = []
threshold = 0.5  # Prediction confidence threshold

cap = cv2.VideoCapture(0)
model.load_weights('action_detectiondem.h5')

with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:
    while cap.isOpened():
        ret, frame = cap.read()
        image, results = mediapipe_detection(frame, holistic)
        draw_landmarks(image, results)

        if results.pose_landmarks and \
   len(results.pose_landmarks.landmark) > 16 and \
   results.pose_landmarks.landmark[15].visibility > 0.5 and \
   results.pose_landmarks.landmark[16].visibility > 0.5:
            keypoints = extract_keypoints(results)
            sequence.append(keypoints)
            sequence = sequence[-30:]  # Keep only last 30 frames

            if len(sequence) == 30:
                res = model.predict(np.expand_dims(sequence, axis=0))[0]
            
            # Use moving average to smooth predictions
                predictions.append(res)
                predictions = predictions[-10:]  # Keep last 10 predictions
            
            # Average predictions
                avg_pred = np.mean(predictions, axis=0)
            
            # Check if max prediction exceeds threshold
                if np.max(avg_pred) > threshold:
                    predicted_action = actions[np.argmax(avg_pred)]
                    cv2.putText(image, f'Action: {predicted_action}', (10, 50), 
                            cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)

            image = prob_viz(res, actions, image, colors)
        cv2.imshow('OpenCV Feed', image)
        
        if cv2.waitKey(10) & 0xFF == ord('q'):
            break

cap.release()
cv2.destroyAllWindows()

